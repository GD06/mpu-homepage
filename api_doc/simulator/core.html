<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.8.3" />
<title>simulator.core API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.18.1/highlight.min.js" integrity="sha256-eOgo0OtLL4cdq7RdwRUiGKLX9XsIJ7nGhWEKbohmVAQ=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>simulator.core</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from copy import deepcopy 

from simulator.processing_group import ProcessingGroup
from simulator.subcore import Subcore 
from simulator.instr_cache import InstrCache
from simulator.subcore_table import WarpInfoTableEntry
from simulator.shared_memory import SharedMemory 
from simulator.subcore_pg_bus import SubcorePGBusArbiter


class Core:

    def __init__(self, core_id, env, config, log, processor):
        self.core_id = core_id
        self.env = env
        self.config = config 
        self.log = log 
        self.processor = processor 

        assert config[&#34;sim_clock_freq&#34;] % config[&#34;core_clock_freq&#34;] == 0, (
            &#34;Undividable simulation clock frequency&#34;)
        self.clock_unit = config[&#34;sim_clock_freq&#34;] // config[&#34;core_clock_freq&#34;]

        self.subcore_pg_bus_arbiter = SubcorePGBusArbiter(
            env=self.env,
            log=self.log,
            config=self.config,
            clock_unit=self.clock_unit,
            core=self
        )

        self.pg_array = []
        for i in range(config[&#34;num_pg&#34;]):
            pg = ProcessingGroup(
                pg_id=i,
                env=env,
                config=config, 
                log=log,
                core=self, 
            )
            self.pg_array.append(pg)

        self.subcore_array = []
        for i in range(config[&#34;num_subcore&#34;]):
            subcore = Subcore(
                subcore_id=i,
                env=env, 
                config=config, 
                log=log,
                core=self, 
            )
            self.subcore_array.append(subcore)

        self.icache = InstrCache(env=env, config=config, log=log, core=self) 
        self.smem = SharedMemory(
            env=env, log=log, clock_unit=self.clock_unit,
            capacity=self.config[&#34;shared_memory_size&#34;],
        )

        self.max_num_bar_ids = (
            self.config[&#34;num_subcore&#34;] 
            * self.config[&#34;max_num_warp_per_subcore&#34;] 
            * self.config[&#34;max_bar_id_per_block&#34;])
        self.bar_count = [0] * self.max_num_bar_ids
        self.bar_release = [False] * self.max_num_bar_ids 

        self.shared_memory_ptr = 0
        self.bar_id_ptr = 0
        self.current_kernel = None
        self.param_dict = None
        self.grid_dim = None 
        self.block_dim = None 

        return

    def _check_shared_memory_usage(self, shared_memory_usage_per_block):
        &#34;&#34;&#34;Check whether the current available shared memory on the core is 
        sufficient to accomodate a new thread block 
        &#34;&#34;&#34;
        new_ptr = self.shared_memory_ptr + shared_memory_usage_per_block 
        if new_ptr &gt; self.config[&#34;shared_memory_size&#34;]:
            return False 
        return True

    def _check_available_resources(
        self, reg_usage_per_warp, shared_memory_usage_per_block, block_dim
    ):
        &#34;&#34;&#34;Check whether the current available hardware resources on the 
        core to accommodate a new thread block.
        
        Args: 
            reg_usage_per_warp: the number of bytes of register file needed by
                a thread warp. 
            shred_memory_usage_per_block: the number of bytes of shared memory
                needed by a thread block.  
            block_dim: the dimension of a thread block. 

        Returns:
            A boolean variable to indicate whether the core is able to 
                schedule a new thread block. 
        &#34;&#34;&#34;
        # Check whether there is sufficient shared memory 
        if not self._check_shared_memory_usage(shared_memory_usage_per_block):
            return False 

        warp_usage_subcores = [] 
        reg_usage_subcores = []
        reg_usage_pgs = []

        for i in range(self.config[&#34;num_subcore&#34;]):
            warp_usage_subcores.append(0)
            reg_usage_subcores.append(0)

        for i in range(self.config[&#34;num_pg&#34;]):
            reg_usage_pgs.append(0)

        warp_id = 0
        for tidz in range(block_dim[0]):
            for tidy in range(block_dim[1]):
                for tidx in range(
                    0, block_dim[2], self.config[&#34;num_threads_per_warp&#34;]
                ):

                    subcore_id = warp_id % self.config[&#34;num_subcore&#34;]
                    pg_id = warp_id % self.config[&#34;num_pg&#34;]

                    warp_usage_subcores[subcore_id] += 1
                    reg_usage_subcores[subcore_id] += reg_usage_per_warp 
                    reg_usage_pgs[pg_id] += reg_usage_per_warp

                    warp_id = warp_id + 1

        for i in range(self.config[&#34;num_subcore&#34;]):
            if not self.subcore_array[i].check_reg_usage(
                reg_usage_subcores[i]
            ):
                return False

            if not self.subcore_array[i].check_warp_usage(
                warp_usage_subcores[i]
            ):
                return False 

        for i in range(self.config[&#34;num_pg&#34;]):
            if not self.pg_array[i].check_reg_usage(reg_usage_pgs[i]):
                return False 

        return True

    def _schedule_thread_block(
        self, reg_usage_per_warp, shared_memory_usage_per_block, 
        block_dim, block_id
    ):
        &#34;&#34;&#34;Allocate harddware resources to schedule a thread block into 
        subcores, and initiate entries needed in the warp table. 

        Args: 
            reg_usage_per_warp: the number of bytes of register file needed by
                a thread warp. 
            shared_memory_usage_per_block: the number of bytes of shared memory 
                needed by a thread block.  
            block_dim: the dimension of a thread block. 
            block_id: a tuple including the ID of a thread block formated as 
                (block_id_z, block_id_y, block_id_x)
        &#34;&#34;&#34;
        # Step 1: fill in warp table accordingly 
        warp_id = 0
        for tidz in range(block_dim[0]):
            for tidy in range(block_dim[1]):
                for tidx in range(
                    0, block_dim[2], self.config[&#34;num_threads_per_warp&#34;]
                ):

                    subcore_id = warp_id % self.config[&#34;num_subcore&#34;]
                    pg_id = warp_id % self.config[&#34;num_pg&#34;]

                    subcore_reg_base_addr = deepcopy(
                        self.subcore_array[subcore_id].reg_base_ptr
                    )
                    pg_reg_base_addr = deepcopy(
                        self.pg_array[pg_id].reg_base_ptr 
                    )
                    smem_base_addr = deepcopy(self.shared_memory_ptr) 
                    bar_id_base_addr = deepcopy(self.bar_id_ptr) 

                    new_warp_info_table_entry = WarpInfoTableEntry(
                        thread_id=(tidz, tidy, tidx),
                        block_id=block_id,
                        pg_id=pg_id, 
                        subcore_reg_base_addr=subcore_reg_base_addr,
                        pg_reg_base_addr=pg_reg_base_addr, 
                        smem_base_addr=smem_base_addr,
                        bar_id_base_addr=bar_id_base_addr, 
                        prog_reg_offset=self.current_kernel.reg_offset,
                        prog_reg_size=self.current_kernel.reg_size,
                        prog_smem_offset=self.current_kernel.smem_offset,
                        prog_length=len(self.current_kernel.instr_list) 
                    )

                    entry_id = deepcopy(
                        self.subcore_array[subcore_id].num_active_warps
                    ) 
                    self.subcore_array[subcore_id].warp_info_table.entry[
                        entry_id] = new_warp_info_table_entry 

                    self.subcore_array[subcore_id].num_active_warps += 1
                    self.subcore_array[subcore_id].reg_base_ptr += (
                        reg_usage_per_warp 
                    )
                    self.pg_array[pg_id].reg_base_ptr += (
                        reg_usage_per_warp
                    )

                    warp_id = warp_id + 1

        # Step 2: increase the shared memory pointer accordingly 
        self.shared_memory_ptr += shared_memory_usage_per_block 

        # Step 3: increase the synchronization barrier pointer accordingly 
        self.bar_id_ptr += self.config[&#34;max_bar_id_per_block&#34;]

        return 

    def _invoke_subcore_simulation(self):
        &#34;&#34;&#34;This function invokes the simulation of subcores and wait until 
        the completion. After that, this function reset the subcore status 
        into its original. 
        &#34;&#34;&#34;
        for i in range(self.config[&#34;num_subcore&#34;]):
            yield self.subcore_array[i].start_exec_cmd.put(&#34;start&#34;)

        for i in range(self.config[&#34;num_subcore&#34;]):
            resp = yield self.subcore_array[i].finish_exec_resp.get()
            assert resp == &#34;success&#34;, &#34;The response {} does not &#34; \
                &#34;indicate a successful execution&#34;.format(resp)

        # Reset the hardware status for every components 
        for i in range(self.config[&#34;num_subcore&#34;]):
            self.subcore_array[i].reset_status() 

        for i in range(self.config[&#34;num_pg&#34;]):
            self.pg_array[i].reset_status() 

        # Release the shared memory 
        self.shared_memory_ptr = 0 

        # Release the synchronization barrier slots 
        self.bar_id_ptr = 0 

        return 

    def run_simulation(self, reg_usage_per_warp, shared_memory_usage_per_block,
                       kernel, param_dict, grid_dim, block_dim, task_list):
        &#34;&#34;&#34;This function schedules thread blocks into subcores when resources
        are available. 

        Args: 
            reg_usage_per_warp: the usage of registers per thread warp. 
            shared_memory_usage_per_block: the usage of shared memory per 
                thread block. 
            kernel: the kernel function to execute. 
            param_dict: the dictionary mapping from parameter names to values. 
            grid_dim: the dimension of a block grid, which is a tuple 
                formatted as (block_id_z, block_id_y, block_id_x)
            block_dim: the dimension of a thread block, which is a tuple 
                formatted as (thread_id_z, thread_id_y, thread_id_x). 
            task_list: a list of thread blocks scheduled to this core. 
        &#34;&#34;&#34; 
        self.current_kernel = kernel 
        self.param_dict = param_dict 
        self.grid_dim = grid_dim 
        self.block_dim = block_dim 

        task_list_length = len(task_list)
        curr_task_id = 0

        while curr_task_id &lt; task_list_length:
            if self._check_available_resources(
                reg_usage_per_warp, 
                shared_memory_usage_per_block, 
                block_dim
            ):

                self._schedule_thread_block(
                    reg_usage_per_warp, shared_memory_usage_per_block, 
                    block_dim, task_list[curr_task_id]
                )
                curr_task_id = curr_task_id + 1

            else:
                # Invoke the subcore simulation until the completion of 
                # scheduled blocks so that resources can be reclaimed to 
                # continue scheduling the next thread block 
                yield self.env.process(self._invoke_subcore_simulation()) 

        yield self.env.process(self._invoke_subcore_simulation())  

        self.current_kernel = None 
        self.param_dict = None 
        self.grid_dim = None 
        self.block_dim = None 

        return </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="simulator.core.Core"><code class="flex name class">
<span>class <span class="ident">Core</span></span>
<span>(</span><span>core_id, env, config, log, processor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Core:

    def __init__(self, core_id, env, config, log, processor):
        self.core_id = core_id
        self.env = env
        self.config = config 
        self.log = log 
        self.processor = processor 

        assert config[&#34;sim_clock_freq&#34;] % config[&#34;core_clock_freq&#34;] == 0, (
            &#34;Undividable simulation clock frequency&#34;)
        self.clock_unit = config[&#34;sim_clock_freq&#34;] // config[&#34;core_clock_freq&#34;]

        self.subcore_pg_bus_arbiter = SubcorePGBusArbiter(
            env=self.env,
            log=self.log,
            config=self.config,
            clock_unit=self.clock_unit,
            core=self
        )

        self.pg_array = []
        for i in range(config[&#34;num_pg&#34;]):
            pg = ProcessingGroup(
                pg_id=i,
                env=env,
                config=config, 
                log=log,
                core=self, 
            )
            self.pg_array.append(pg)

        self.subcore_array = []
        for i in range(config[&#34;num_subcore&#34;]):
            subcore = Subcore(
                subcore_id=i,
                env=env, 
                config=config, 
                log=log,
                core=self, 
            )
            self.subcore_array.append(subcore)

        self.icache = InstrCache(env=env, config=config, log=log, core=self) 
        self.smem = SharedMemory(
            env=env, log=log, clock_unit=self.clock_unit,
            capacity=self.config[&#34;shared_memory_size&#34;],
        )

        self.max_num_bar_ids = (
            self.config[&#34;num_subcore&#34;] 
            * self.config[&#34;max_num_warp_per_subcore&#34;] 
            * self.config[&#34;max_bar_id_per_block&#34;])
        self.bar_count = [0] * self.max_num_bar_ids
        self.bar_release = [False] * self.max_num_bar_ids 

        self.shared_memory_ptr = 0
        self.bar_id_ptr = 0
        self.current_kernel = None
        self.param_dict = None
        self.grid_dim = None 
        self.block_dim = None 

        return

    def _check_shared_memory_usage(self, shared_memory_usage_per_block):
        &#34;&#34;&#34;Check whether the current available shared memory on the core is 
        sufficient to accomodate a new thread block 
        &#34;&#34;&#34;
        new_ptr = self.shared_memory_ptr + shared_memory_usage_per_block 
        if new_ptr &gt; self.config[&#34;shared_memory_size&#34;]:
            return False 
        return True

    def _check_available_resources(
        self, reg_usage_per_warp, shared_memory_usage_per_block, block_dim
    ):
        &#34;&#34;&#34;Check whether the current available hardware resources on the 
        core to accommodate a new thread block.
        
        Args: 
            reg_usage_per_warp: the number of bytes of register file needed by
                a thread warp. 
            shred_memory_usage_per_block: the number of bytes of shared memory
                needed by a thread block.  
            block_dim: the dimension of a thread block. 

        Returns:
            A boolean variable to indicate whether the core is able to 
                schedule a new thread block. 
        &#34;&#34;&#34;
        # Check whether there is sufficient shared memory 
        if not self._check_shared_memory_usage(shared_memory_usage_per_block):
            return False 

        warp_usage_subcores = [] 
        reg_usage_subcores = []
        reg_usage_pgs = []

        for i in range(self.config[&#34;num_subcore&#34;]):
            warp_usage_subcores.append(0)
            reg_usage_subcores.append(0)

        for i in range(self.config[&#34;num_pg&#34;]):
            reg_usage_pgs.append(0)

        warp_id = 0
        for tidz in range(block_dim[0]):
            for tidy in range(block_dim[1]):
                for tidx in range(
                    0, block_dim[2], self.config[&#34;num_threads_per_warp&#34;]
                ):

                    subcore_id = warp_id % self.config[&#34;num_subcore&#34;]
                    pg_id = warp_id % self.config[&#34;num_pg&#34;]

                    warp_usage_subcores[subcore_id] += 1
                    reg_usage_subcores[subcore_id] += reg_usage_per_warp 
                    reg_usage_pgs[pg_id] += reg_usage_per_warp

                    warp_id = warp_id + 1

        for i in range(self.config[&#34;num_subcore&#34;]):
            if not self.subcore_array[i].check_reg_usage(
                reg_usage_subcores[i]
            ):
                return False

            if not self.subcore_array[i].check_warp_usage(
                warp_usage_subcores[i]
            ):
                return False 

        for i in range(self.config[&#34;num_pg&#34;]):
            if not self.pg_array[i].check_reg_usage(reg_usage_pgs[i]):
                return False 

        return True

    def _schedule_thread_block(
        self, reg_usage_per_warp, shared_memory_usage_per_block, 
        block_dim, block_id
    ):
        &#34;&#34;&#34;Allocate harddware resources to schedule a thread block into 
        subcores, and initiate entries needed in the warp table. 

        Args: 
            reg_usage_per_warp: the number of bytes of register file needed by
                a thread warp. 
            shared_memory_usage_per_block: the number of bytes of shared memory 
                needed by a thread block.  
            block_dim: the dimension of a thread block. 
            block_id: a tuple including the ID of a thread block formated as 
                (block_id_z, block_id_y, block_id_x)
        &#34;&#34;&#34;
        # Step 1: fill in warp table accordingly 
        warp_id = 0
        for tidz in range(block_dim[0]):
            for tidy in range(block_dim[1]):
                for tidx in range(
                    0, block_dim[2], self.config[&#34;num_threads_per_warp&#34;]
                ):

                    subcore_id = warp_id % self.config[&#34;num_subcore&#34;]
                    pg_id = warp_id % self.config[&#34;num_pg&#34;]

                    subcore_reg_base_addr = deepcopy(
                        self.subcore_array[subcore_id].reg_base_ptr
                    )
                    pg_reg_base_addr = deepcopy(
                        self.pg_array[pg_id].reg_base_ptr 
                    )
                    smem_base_addr = deepcopy(self.shared_memory_ptr) 
                    bar_id_base_addr = deepcopy(self.bar_id_ptr) 

                    new_warp_info_table_entry = WarpInfoTableEntry(
                        thread_id=(tidz, tidy, tidx),
                        block_id=block_id,
                        pg_id=pg_id, 
                        subcore_reg_base_addr=subcore_reg_base_addr,
                        pg_reg_base_addr=pg_reg_base_addr, 
                        smem_base_addr=smem_base_addr,
                        bar_id_base_addr=bar_id_base_addr, 
                        prog_reg_offset=self.current_kernel.reg_offset,
                        prog_reg_size=self.current_kernel.reg_size,
                        prog_smem_offset=self.current_kernel.smem_offset,
                        prog_length=len(self.current_kernel.instr_list) 
                    )

                    entry_id = deepcopy(
                        self.subcore_array[subcore_id].num_active_warps
                    ) 
                    self.subcore_array[subcore_id].warp_info_table.entry[
                        entry_id] = new_warp_info_table_entry 

                    self.subcore_array[subcore_id].num_active_warps += 1
                    self.subcore_array[subcore_id].reg_base_ptr += (
                        reg_usage_per_warp 
                    )
                    self.pg_array[pg_id].reg_base_ptr += (
                        reg_usage_per_warp
                    )

                    warp_id = warp_id + 1

        # Step 2: increase the shared memory pointer accordingly 
        self.shared_memory_ptr += shared_memory_usage_per_block 

        # Step 3: increase the synchronization barrier pointer accordingly 
        self.bar_id_ptr += self.config[&#34;max_bar_id_per_block&#34;]

        return 

    def _invoke_subcore_simulation(self):
        &#34;&#34;&#34;This function invokes the simulation of subcores and wait until 
        the completion. After that, this function reset the subcore status 
        into its original. 
        &#34;&#34;&#34;
        for i in range(self.config[&#34;num_subcore&#34;]):
            yield self.subcore_array[i].start_exec_cmd.put(&#34;start&#34;)

        for i in range(self.config[&#34;num_subcore&#34;]):
            resp = yield self.subcore_array[i].finish_exec_resp.get()
            assert resp == &#34;success&#34;, &#34;The response {} does not &#34; \
                &#34;indicate a successful execution&#34;.format(resp)

        # Reset the hardware status for every components 
        for i in range(self.config[&#34;num_subcore&#34;]):
            self.subcore_array[i].reset_status() 

        for i in range(self.config[&#34;num_pg&#34;]):
            self.pg_array[i].reset_status() 

        # Release the shared memory 
        self.shared_memory_ptr = 0 

        # Release the synchronization barrier slots 
        self.bar_id_ptr = 0 

        return 

    def run_simulation(self, reg_usage_per_warp, shared_memory_usage_per_block,
                       kernel, param_dict, grid_dim, block_dim, task_list):
        &#34;&#34;&#34;This function schedules thread blocks into subcores when resources
        are available. 

        Args: 
            reg_usage_per_warp: the usage of registers per thread warp. 
            shared_memory_usage_per_block: the usage of shared memory per 
                thread block. 
            kernel: the kernel function to execute. 
            param_dict: the dictionary mapping from parameter names to values. 
            grid_dim: the dimension of a block grid, which is a tuple 
                formatted as (block_id_z, block_id_y, block_id_x)
            block_dim: the dimension of a thread block, which is a tuple 
                formatted as (thread_id_z, thread_id_y, thread_id_x). 
            task_list: a list of thread blocks scheduled to this core. 
        &#34;&#34;&#34; 
        self.current_kernel = kernel 
        self.param_dict = param_dict 
        self.grid_dim = grid_dim 
        self.block_dim = block_dim 

        task_list_length = len(task_list)
        curr_task_id = 0

        while curr_task_id &lt; task_list_length:
            if self._check_available_resources(
                reg_usage_per_warp, 
                shared_memory_usage_per_block, 
                block_dim
            ):

                self._schedule_thread_block(
                    reg_usage_per_warp, shared_memory_usage_per_block, 
                    block_dim, task_list[curr_task_id]
                )
                curr_task_id = curr_task_id + 1

            else:
                # Invoke the subcore simulation until the completion of 
                # scheduled blocks so that resources can be reclaimed to 
                # continue scheduling the next thread block 
                yield self.env.process(self._invoke_subcore_simulation()) 

        yield self.env.process(self._invoke_subcore_simulation())  

        self.current_kernel = None 
        self.param_dict = None 
        self.grid_dim = None 
        self.block_dim = None 

        return </code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="simulator.core.Core.run_simulation"><code class="name flex">
<span>def <span class="ident">run_simulation</span></span>(<span>self, reg_usage_per_warp, shared_memory_usage_per_block, kernel, param_dict, grid_dim, block_dim, task_list)</span>
</code></dt>
<dd>
<div class="desc"><p>This function schedules thread blocks into subcores when resources
are available. </p>
<p>Args:
reg_usage_per_warp: the usage of registers per thread warp.
shared_memory_usage_per_block: the usage of shared memory per
thread block.
kernel: the kernel function to execute.
param_dict: the dictionary mapping from parameter names to values.
grid_dim: the dimension of a block grid, which is a tuple
formatted as (block_id_z, block_id_y, block_id_x)
block_dim: the dimension of a thread block, which is a tuple
formatted as (thread_id_z, thread_id_y, thread_id_x).
task_list: a list of thread blocks scheduled to this core.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def run_simulation(self, reg_usage_per_warp, shared_memory_usage_per_block,
                   kernel, param_dict, grid_dim, block_dim, task_list):
    &#34;&#34;&#34;This function schedules thread blocks into subcores when resources
    are available. 

    Args: 
        reg_usage_per_warp: the usage of registers per thread warp. 
        shared_memory_usage_per_block: the usage of shared memory per 
            thread block. 
        kernel: the kernel function to execute. 
        param_dict: the dictionary mapping from parameter names to values. 
        grid_dim: the dimension of a block grid, which is a tuple 
            formatted as (block_id_z, block_id_y, block_id_x)
        block_dim: the dimension of a thread block, which is a tuple 
            formatted as (thread_id_z, thread_id_y, thread_id_x). 
        task_list: a list of thread blocks scheduled to this core. 
    &#34;&#34;&#34; 
    self.current_kernel = kernel 
    self.param_dict = param_dict 
    self.grid_dim = grid_dim 
    self.block_dim = block_dim 

    task_list_length = len(task_list)
    curr_task_id = 0

    while curr_task_id &lt; task_list_length:
        if self._check_available_resources(
            reg_usage_per_warp, 
            shared_memory_usage_per_block, 
            block_dim
        ):

            self._schedule_thread_block(
                reg_usage_per_warp, shared_memory_usage_per_block, 
                block_dim, task_list[curr_task_id]
            )
            curr_task_id = curr_task_id + 1

        else:
            # Invoke the subcore simulation until the completion of 
            # scheduled blocks so that resources can be reclaimed to 
            # continue scheduling the next thread block 
            yield self.env.process(self._invoke_subcore_simulation()) 

    yield self.env.process(self._invoke_subcore_simulation())  

    self.current_kernel = None 
    self.param_dict = None 
    self.grid_dim = None 
    self.block_dim = None 

    return </code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="simulator" href="index.html">simulator</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="simulator.core.Core" href="#simulator.core.Core">Core</a></code></h4>
<ul class="">
<li><code><a title="simulator.core.Core.run_simulation" href="#simulator.core.Core.run_simulation">run_simulation</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.8.3</a>.</p>
</footer>
</body>
</html>